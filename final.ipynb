{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1670570620215,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "jQShcr37KxOD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traing_data_dir\n",
    "data_dir = 'crops_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670570620658,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "38bLoLSjLbOv"
   },
   "outputs": [],
   "source": [
    "# original_data_dir\n",
    "dir = 'crops_image'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAl5UcSJPlCD"
   },
   "source": [
    "# Spiltting the images into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = {}\n",
    "\n",
    "# for folder in os.listdir(dir):\n",
    "#     image_path = os.path.join(dir, folder) \n",
    "#     images = []\n",
    "#     for image in os.listdir(image_path):\n",
    "#         if image.endswith('.PNG') or image.endswith('.png'):\n",
    "#             images.append(image)\n",
    "        \n",
    "#     classes[folder] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 將原始資料複製到新的資料夾，並分成train,test 兩個資料夾\n",
    "\n",
    "# print(os.getcwd())\n",
    "# for key in classes.keys():\n",
    "#     random.shuffle(classes[key])\n",
    "\n",
    "#     # train : test = 8 : 2 \n",
    "#     point1 = int(len(classes[key])*0.8)\n",
    "    \n",
    "#     for name in ['train', 'test']:\n",
    "#         save_path = os.path.join(data_dir, name, key)\n",
    "#         os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "#         if name == 'train':\n",
    "#             for image in classes[key][:point1]:\n",
    "#                 src = os.path.join(dir, key, image)\n",
    "#                 dst = os.path.join(save_path, image)\n",
    "#                 shutil.copy(src, dst)\n",
    "\n",
    "#         if name == 'test':\n",
    "#             for image in classes[key][point1:]:\n",
    "#                 src = os.path.join(dir, key, image)\n",
    "#                 dst = os.path.join(save_path, image)\n",
    "#                 shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKf_McDNPwzf"
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1670570623947,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "MRyrPrvkcDjl",
    "outputId": "f07ca319-caff-4346-f1a4-49ad3f4f953f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18,resnet34,resnet50,resnet101,resnet152\n",
    "import torchvision\n",
    "print(torch.cuda.is_available())\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1670570624400,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "-m9LtJO3hcrk"
   },
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "# model = resnet18(pretrained=True)\n",
    "# model = resnet34(pretrained=True)\n",
    "# model = resnet101(pretrained=True)\n",
    "# model = resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MN0H8NKPKvW"
   },
   "source": [
    "### Data Augumentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1670570624401,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "jbq3FDq-honp"
   },
   "outputs": [],
   "source": [
    "# Set input image size\n",
    "input_size = (224, 224)\n",
    " \n",
    "# Set the data directory\n",
    "\n",
    "\n",
    "# Define data transformation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSKzCWyXQXsC"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent error\n",
    "def custom_loader(path):\n",
    "    try:\n",
    "        return Image.open(path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {path}: {e}\")\n",
    "        # Return a blank image or handle it in a way that suits your needs\n",
    "        # we turn the wrong picture into a completely white image \n",
    "        return Image.new('RGB', (224, 224), (255, 255, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1670570624401,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "F99Dnovulmw0"
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    image_datasets = {x: ImageFolder(os.path.join(data_dir, x),\n",
    "                        transform=data_transforms[x],loader=custom_loader) for x in ['train', 'test']}\n",
    "\n",
    "    return image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset ImageFolder\n",
      "    Number of datapoints: 2700\n",
      "    Root location: crops_data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomVerticalFlip(p=0.5)\n",
      "               ColorJitter(brightness=[0.5, 1.5], contrast=None, saturation=None, hue=None)\n",
      "               ToTensor()\n",
      "           ), 'test': Dataset ImageFolder\n",
      "    Number of datapoints: 660\n",
      "    Root location: crops_data/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "           )}\n"
     ]
    }
   ],
   "source": [
    "print(load_data(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1670570624401,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "NhQQane4lp8c"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1670570624402,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "hLhNEUVIltZ2",
    "outputId": "82d57c0f-5551-4080-880b-4869f1a48df7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 19 23:23:24 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   41C    P8    17W / 280W |     10MiB / 11177MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "|  0%   35C    P8     9W / 280W |     10MiB / 11178MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFBv5v7DQtLJ"
   },
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/home/nas/Research_Group/Personal/Hamson/DS_code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670570624402,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "KI9b3cnfl4b9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def train(config, model):\n",
    "    \n",
    "    p = os.path.join(save_path, 'checkpoint_epoch100')\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # set the criterion, using the cross-entropy as the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Choose a optimizer you want (e.g. Adam...)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9) \n",
    "\n",
    "    # load data\n",
    "    datasets = load_data(config[\"data_dir\"])\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(datasets['train'], batch_size=int(16),\n",
    "                    shuffle=True, drop_last=True, num_workers=8),\n",
    "                   'test': torch.utils.data.DataLoader(datasets['test'], batch_size=int(16),\n",
    "                    shuffle=True, drop_last=True, num_workers=8),}\n",
    "    \n",
    "    # store the accuracy\n",
    "    best_acc = 0.0\n",
    "    epoch = 0\n",
    "    \n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "    \n",
    "    while True:\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            total = 0\n",
    "            running_loss, running_corrects = 0.0, 0.0\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                total += labels.size(0)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss / total\n",
    "                train_acc = running_corrects / total\n",
    "                precision = precision_score(all_labels, all_preds, average='macro')\n",
    "                recall = recall_score(all_labels, all_preds, average='macro')\n",
    "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "                \n",
    "            elif phase == 'test':\n",
    "                val_loss = running_loss / total\n",
    "                val_acc = running_corrects / total\n",
    "                precision = precision_score(all_labels, all_preds, average='macro')\n",
    "                recall = recall_score(all_labels, all_preds, average='macro')\n",
    "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "       \n",
    "        print(f'Epoch:{epoch}')\n",
    "        print(f'Training accuracy: {train_acc:.4f}, training loss: {train_loss:.4f}')\n",
    "        print(f'Validation accuracy: {val_acc:.4f}, validation loss: {val_loss:.4f}')\n",
    "        print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "        \n",
    "        val_loss_history.append(val_loss)\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # save the best model\n",
    "        if epoch == 0 or val_acc >= best_acc:\n",
    "            best_acc = train_acc\n",
    "            path = os.path.join(p, f'checkpoint_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, path)            \n",
    "        \n",
    "        if epoch == config[\"epoch\"]:\n",
    "            break\n",
    "        else:\n",
    "            epoch = epoch + 1\n",
    "\n",
    "    print(f'Finish training! Best accuracy: {best_acc}') \n",
    "    return val_loss_history, train_loss_history, val_acc_history, train_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670570624402,
     "user": {
      "displayName": "謝宇倫",
      "userId": "08023474272854488958"
     },
     "user_tz": -480
    },
    "id": "-9yMhKIcHk-_"
   },
   "outputs": [],
   "source": [
    "# set the training detail\n",
    "\n",
    "training_config = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"epoch\": 40,\n",
    "    \"data_dir\": data_dir\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXp63TLBH0Ix",
    "outputId": "34c0de8a-d6c8-4709-efc5-bd9f977f08a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:0\n",
      "Training accuracy: 0.3363, training loss: 0.2550\n",
      "Validation accuracy: 0.7348, validation loss: 0.0744\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:1\n",
      "Training accuracy: 0.7061, training loss: 0.0679\n",
      "Validation accuracy: 0.8354, validation loss: 0.0364\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:2\n",
      "Training accuracy: 0.7898, training loss: 0.0440\n",
      "Validation accuracy: 0.8811, validation loss: 0.0253\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:3\n",
      "Training accuracy: 0.8166, training loss: 0.0376\n",
      "Validation accuracy: 0.8918, validation loss: 0.0204\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:4\n",
      "Training accuracy: 0.8486, training loss: 0.0305\n",
      "Validation accuracy: 0.9024, validation loss: 0.0186\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:5\n",
      "Training accuracy: 0.8717, training loss: 0.0265\n",
      "Validation accuracy: 0.9101, validation loss: 0.0160\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:6\n",
      "Training accuracy: 0.8646, training loss: 0.0257\n",
      "Validation accuracy: 0.9162, validation loss: 0.0152\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:7\n",
      "Training accuracy: 0.8940, training loss: 0.0214\n",
      "Validation accuracy: 0.9223, validation loss: 0.0138\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:8\n",
      "Training accuracy: 0.8984, training loss: 0.0198\n",
      "Validation accuracy: 0.9284, validation loss: 0.0125\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:9\n",
      "Training accuracy: 0.9066, training loss: 0.0193\n",
      "Validation accuracy: 0.9314, validation loss: 0.0121\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:10\n",
      "Training accuracy: 0.9163, training loss: 0.0177\n",
      "Validation accuracy: 0.9345, validation loss: 0.0126\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:11\n",
      "Training accuracy: 0.9152, training loss: 0.0171\n",
      "Validation accuracy: 0.9390, validation loss: 0.0123\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:12\n",
      "Training accuracy: 0.9301, training loss: 0.0141\n",
      "Validation accuracy: 0.9482, validation loss: 0.0111\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:13\n",
      "Training accuracy: 0.9275, training loss: 0.0156\n",
      "Validation accuracy: 0.9405, validation loss: 0.0110\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:14\n",
      "Training accuracy: 0.9427, training loss: 0.0127\n",
      "Validation accuracy: 0.9405, validation loss: 0.0116\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:15\n",
      "Training accuracy: 0.9401, training loss: 0.0123\n",
      "Validation accuracy: 0.9451, validation loss: 0.0107\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:16\n",
      "Training accuracy: 0.9464, training loss: 0.0116\n",
      "Validation accuracy: 0.9482, validation loss: 0.0109\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:17\n",
      "Training accuracy: 0.9550, training loss: 0.0101\n",
      "Validation accuracy: 0.9436, validation loss: 0.0111\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:18\n",
      "Training accuracy: 0.9435, training loss: 0.0111\n",
      "Validation accuracy: 0.9466, validation loss: 0.0109\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:19\n",
      "Training accuracy: 0.9520, training loss: 0.0105\n",
      "Validation accuracy: 0.9436, validation loss: 0.0110\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:20\n",
      "Training accuracy: 0.9568, training loss: 0.0093\n",
      "Validation accuracy: 0.9451, validation loss: 0.0114\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:21\n",
      "Training accuracy: 0.9572, training loss: 0.0093\n",
      "Validation accuracy: 0.9466, validation loss: 0.0108\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:22\n",
      "Training accuracy: 0.9624, training loss: 0.0087\n",
      "Validation accuracy: 0.9436, validation loss: 0.0104\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:23\n",
      "Training accuracy: 0.9699, training loss: 0.0071\n",
      "Validation accuracy: 0.9497, validation loss: 0.0101\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:24\n",
      "Training accuracy: 0.9658, training loss: 0.0079\n",
      "Validation accuracy: 0.9405, validation loss: 0.0118\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:25\n",
      "Training accuracy: 0.9661, training loss: 0.0076\n",
      "Validation accuracy: 0.9482, validation loss: 0.0107\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:26\n",
      "Training accuracy: 0.9676, training loss: 0.0075\n",
      "Validation accuracy: 0.9466, validation loss: 0.0101\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:27\n",
      "Training accuracy: 0.9747, training loss: 0.0061\n",
      "Validation accuracy: 0.9497, validation loss: 0.0096\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:28\n",
      "Training accuracy: 0.9684, training loss: 0.0064\n",
      "Validation accuracy: 0.9558, validation loss: 0.0102\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:29\n",
      "Training accuracy: 0.9743, training loss: 0.0061\n",
      "Validation accuracy: 0.9512, validation loss: 0.0099\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:30\n",
      "Training accuracy: 0.9792, training loss: 0.0052\n",
      "Validation accuracy: 0.9482, validation loss: 0.0112\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:31\n",
      "Training accuracy: 0.9740, training loss: 0.0066\n",
      "Validation accuracy: 0.9482, validation loss: 0.0103\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:32\n",
      "Training accuracy: 0.9714, training loss: 0.0060\n",
      "Validation accuracy: 0.9497, validation loss: 0.0100\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:33\n",
      "Training accuracy: 0.9758, training loss: 0.0055\n",
      "Validation accuracy: 0.9466, validation loss: 0.0108\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:34\n",
      "Training accuracy: 0.9803, training loss: 0.0052\n",
      "Validation accuracy: 0.9512, validation loss: 0.0103\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:35\n",
      "Training accuracy: 0.9777, training loss: 0.0050\n",
      "Validation accuracy: 0.9497, validation loss: 0.0100\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:36\n",
      "Training accuracy: 0.9799, training loss: 0.0052\n",
      "Validation accuracy: 0.9527, validation loss: 0.0093\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:37\n",
      "Training accuracy: 0.9807, training loss: 0.0046\n",
      "Validation accuracy: 0.9604, validation loss: 0.0093\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:38\n",
      "Training accuracy: 0.9829, training loss: 0.0044\n",
      "Validation accuracy: 0.9634, validation loss: 0.0085\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:39\n",
      "Training accuracy: 0.9851, training loss: 0.0037\n",
      "Validation accuracy: 0.9558, validation loss: 0.0092\n",
      "Error loading image crops_data/train/atemoya/atemoya.38.png: image file is truncated\n",
      "Epoch:40\n",
      "Training accuracy: 0.9825, training loss: 0.0042\n",
      "Validation accuracy: 0.9588, validation loss: 0.0086\n",
      "Finish training! Best accuray: 0.9683779761904762\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "val_loss, train_loss, val_acc, train_acc = train(training_config, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "testset = ImageFolder(os.path.join(data_dir, 'test'), transform=data_transforms['test'])\n",
    "\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=int(16),\n",
    "                                        shuffle=False, num_workers=2)\n",
    "device = \"cuda:0\"\n",
    "# iterate over test data\n",
    "for inputs, labels in testloader:\n",
    "    inputs = inputs.to(device)\n",
    "    output = model(inputs) # Feed Network\n",
    "\n",
    "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "    y_pred.extend(output) # Save Prediction\n",
    "\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = ('atemoya', 'banana', 'bareland', 'cabbage', 'carrot',\n",
    "        'grapes', 'guava', 'mango', 'papaya', 'pineapple',\n",
    "        'pumpkin')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(cf_matrix)\n",
    "cf_matrix = confusion_matrix(y_true, y_pred, normalize = \"true\")\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) * 11, index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sns.heatmap(df_cm, cmap='Greens', annot=True)\n",
    "plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "XAl5UcSJPlCD"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
